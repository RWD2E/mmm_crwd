{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder()\n",
    "# spark = SparkSession.newSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where clause for identifing pregnancy events using ICD codes\n",
    "def cd_where_clause(cd_type, cd_lst:list, alias=''):\n",
    "    '''\n",
    "    sql where clause for cohort selection based on ICD code list\n",
    "    '''\n",
    "    cd_field_map = {\n",
    "        'dx':'conditioncode',\n",
    "        'px':'procedurecode'\n",
    "    }\n",
    "    if alias is not '':\n",
    "        alias += '.'\n",
    "    cd_quote = []\n",
    "    for code in cd_lst:\n",
    "        cd_quote.append(\"'\"+ code +\"'\")\n",
    "    cd_quote_str = \",\".join(cd_quote)\n",
    "    sql_str = \"substring_index(upper(\"+ alias + cd_field_map[cd_type] +\".standard.id),'.',1) in (\" + cd_quote_str +\")\"\n",
    "    return sql_str \n",
    "\n",
    "# where clause for identifing pregnancy events using ICD codes\n",
    "preg_icd = [str(x) for x in list(range(630,679,1))]\n",
    "preg_icd.extend(['V22','V23','V28','Z33','Z34','Z35','Z36','Z37','Z38','Z3A','O9A'])\n",
    "preg_icd.extend(['O0' + str(x) for x in list(range(0,9,1))])\n",
    "preg_icd.extend(['O' + str(x) for x in list(range(10,99,1))])\n",
    "preg_where_clause = cd_where_clause(\n",
    "    cd_type = 'dx',\n",
    "    cd_lst = preg_icd,\n",
    "    alias = \"cond\"\n",
    ") \n",
    "preg_where_clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"USE real_world_data_sep_2022\")\n",
    "preg_pt_init = spark.sql('''\n",
    "    select distinct\n",
    "           cond.personid,\n",
    "           to_date(demo.birthdate) as dob,\n",
    "           to_date(demo.dateofdeath) as dod,\n",
    "           coalesce(demo.deceased,false) as deceased,\n",
    "           demo.gender.standard.id as gender,\n",
    "           coalesce(demo.races[0].standard.primaryDisplay,'NI') as race,\n",
    "           coalesce(demo.ethnicities[0].standard.primaryDisplay,'NI') as ethnicity,\n",
    "           cond.encounterid,\n",
    "           cond.conditioncode,\n",
    "           to_date(cond.effectivedate) as dxdate,\n",
    "           cast(round(datediff(to_date(cond.effectivedate),to_date(demo.birthdate))/365.25) as int) as ageatdx,\n",
    "           coalesce(enc.classification.standard.id,'NI') as enctype,\n",
    "           to_date(enc.actualarrivaldate) as admitdate,\n",
    "           date_add(to_date(enc.actualarrivaldate),365) as admitdate_add1yr,\n",
    "           row_number() over (partition by cond.personid order by enc.actualarrivaldate,cond.effectivedate) as rn,\n",
    "           cast(round(datediff(to_date(enc.actualarrivaldate),to_date(demo.birthdate))/365.25) as int) as ageatenc,\n",
    "           enc.readmission,\n",
    "           to_date(enc.dischargedate) as dischargedate\n",
    "    from condition cond\n",
    "    join encounter enc on cond.encounterid = enc.encounterid and enc.classification.standard.id = 'I'\n",
    "    join demographics demo on cond.personid = demo.personid and demo.gender.standard.id = 'F'\n",
    "    where coalesce(to_date(cond.effectivedate),to_date(enc.actualarrivaldate)) between to_date('2010-01-01') and to_date('2022-12-31') and\n",
    "          cast(round(datediff(to_date(enc.actualarrivaldate),to_date(demo.birthdate))/365.25) as int) between 12 and 55\n",
    "'''\n",
    ").cache()\n",
    "preg_pt_init.createOrReplaceTempView(\"preg_pt_init\")\n",
    "preg_pt_init.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"USE real_world_data_jun_2022\")\n",
    "preg_pt_init = spark.sql('''\n",
    "    select distinct\n",
    "           cond.personid,\n",
    "           to_date(demo.birthdate) as dob,\n",
    "           to_date(demo.dateofdeath) as dod,\n",
    "           demo.deceased as deceased,\n",
    "           demo.gender.standard.id as gender,\n",
    "           demo.races[0].standard.primaryDisplay as race,\n",
    "           demo.ethnicities[0].standard.primaryDisplay as ethnicity,\n",
    "           cond.encounterid,\n",
    "           to_date(cond.effectivedate) as dxdate,\n",
    "           cast(round(datediff(to_date(cond.effectivedate),to_date(demo.birthdate))/365.25) as int) as ageatdx,\n",
    "           enc.classification.standard.id as enctype,\n",
    "           to_date(enc.actualarrivaldate) as admitdate,\n",
    "           cast(round(datediff(to_date(enc.actualarrivaldate),to_date(demo.birthdate))/365.25) as int) as ageatenc,\n",
    "           enc.readmission,\n",
    "           to_date(enc.dischargedate) as dischargedate,\n",
    "           enc.tenant as enctenant,\n",
    "           tnt.bed_size as bedsize,\n",
    "           tnt.speciality,\n",
    "           tnt.segment\n",
    "    from condition cond\n",
    "    join encounter enc on cond.encounterid = enc.encounterid\n",
    "    join demographics demo on cond.personid = demo.personid\n",
    "    left join tenant_attributes tnt on enc.tenant = tnt.tenant \n",
    "    where ''' + preg_where_clause\n",
    ").cache()\n",
    "preg_pt_init.createOrReplaceTempView(\"preg_pt_init\")\n",
    "preg_pt_init.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def pt_freq_qry(df,stratified_by,n_way=1):\n",
    "    '''\n",
    "    generate total patient counts for each stratified variables\n",
    "    '''\n",
    "    sql_str_lst = []\n",
    "    # overall count\n",
    "    sql_str_lst.append(\"select 'total' as summ_var,'N' as summ_cat, count(distinct personid) as pat_cnt from \" + df)\n",
    "    # 1-way summary\n",
    "    for var_str in stratified_by:\n",
    "        sql_str_lst.append(\n",
    "            \"select '\" + var_str +\"' as summ_var,\" \n",
    "            + \"cast(\" + var_str +\" as string) as summ_cat,\"\n",
    "            + \"count(distinct personid) as pat_cnt \"\n",
    "            + \"from \"+ df + \" group by \"+ var_str\n",
    "        )\n",
    "        \n",
    "    # n-way summary\n",
    "    if n_way > 1:\n",
    "        for L in range(2,n_way+1,1):\n",
    "            for var_str_comb in itertools.combinations(stratified_by, L):\n",
    "                var_str_concat_by_underline = \"_\".join(var_str_comb)\n",
    "                var_str_concat_by_pipe = \"|| '|' ||\".join(var_str_comb)\n",
    "                var_str_concat_by_comma = \",\".join(var_str_comb)\n",
    "                sql_str_lst.append(\n",
    "                    \"select 'by_\" + var_str_concat_by_underline +\"' as summ_var,\" \n",
    "                    + \"cast(\" + var_str_concat_by_pipe +\" as string) as summ_cat,\"\n",
    "                    + \"count(distinct personid) as pat_cnt \"\n",
    "                    + \"from \"+ df + \" group by \"+ var_str_concat_by_comma\n",
    "                )\n",
    "    \n",
    "    # union everything\n",
    "    sql_str = \" union \".join(sql_str_lst)\n",
    "    return(sql_str)\n",
    "\n",
    "stratified_by = [\n",
    "    'gender',\n",
    "    'race',\n",
    "    'ethnicity',\n",
    "    'segment',\n",
    "    'speciality',\n",
    "    'bedsize',\n",
    "    'deceased_ind'\n",
    "]\n",
    "get_pt_summ = pt_freq_qry('preg_pt_init',stratified_by,n_way=2)\n",
    "get_pt_summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pt_summ = pt_freq_qry(\n",
    "    'preg_pt_init',stratified_by,n_way=2\n",
    ")\n",
    "summ_stat_long = spark.sql(get_pt_summ).toPandas()\n",
    "summ_stat_long.to_csv('./data/summ_stat.csv',index=False)\n",
    "summ_stat_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"USE real_world_data_jun_2022\")\n",
    "preg_pt_12to55 = spark.sql('''\n",
    "    select a.*, extract(year from a.dod) as dod_calyr\n",
    "    from preg_pt_init a\n",
    "    where coalesce(a.dxdate,a.admitdate) between to_date('2010-01-01') and to_date('2022-12-31') and\n",
    "          coalesce(a.ageatdx,a.ageatenc) between 12 and 55\n",
    "''').cache()\n",
    "preg_pt_12to55.createOrReplaceTempView(\"preg_pt_12to55\")\n",
    "preg_pt_12to55.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_by.append('dod_calyr')\n",
    "get_pt_summ = pt_freq_qry(\n",
    "    'preg_pt_12to55',stratified_by,n_way=2\n",
    ")\n",
    "summ_stat_long = spark.sql(get_pt_summ).toPandas()\n",
    "summ_stat_long.to_csv('./data/summ_stat_12to55.csv',index=False)\n",
    "summ_stat_long"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
